{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data, InMemoryDataset\n",
    "import torch_geometric.nn as nn\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "labels = ['addr_contract', 'caller', 'msgvalue', 'balance', 'call_data', 'blk', 'mdata', 'sdata', 'create', 'call', 'callcode', 'delegatecall', 'create2', 'staticcall', 'cal_res', 'comp_res', 'bit_res', 'size', 'code', 'gas', 'return', 'coinbase', 'gasremain', 'revert', 'selfdestruct', 'memory', 'storage', 'flowcontrol']\n",
    "node_types = ['ADDRESS', 'ORIGIN', 'CALLER', 'CALLVALUE', 'BALANCE', 'SELFBALANCE', 'CALLDATALOAD', 'CALLDATACOPY', 'BLOCKHASH', 'TIMESTAMP', 'NUMBER', 'DIFFICULTY', 'BASEFEE', 'MLOAD', 'SLOAD', 'CREATE', 'CALL', 'CALLCODE', 'DELEGATECALL', 'CREATE2', 'STATICCALL', 'ADD', 'MUL', 'SUB', 'EXP', 'LT', 'GT', 'SLT', 'SGT', 'EQ', 'ISZERO', 'AND', 'OR', 'XOR', 'NOT', 'SHL', 'CALLDATASIZE', 'CODESIZE', 'EXTCODESIZE', 'RETURNDATASIZE', 'MSIZE', 'CODECOPY', 'EXTCODECOPY', 'EXTCODEHASH', 'GASPRICE', 'GASLIMIT', 'RETURNDATACOPY', 'RETURN', 'COINBASE', 'GAS', 'REVERT', 'SELFDESTRUCT', 'MSTORE', 'MSTORE8', 'SSTORE', 'JUMP', 'JUMPI', 'JUMPDEST', 'STOP', 'DIV', 'SDIV', 'MOD', 'SMOD', 'ADDMOD', 'SIGNEXTEND', 'BYTE', 'SHR', 'SAR', 'SHA3', 'CHAINID', 'POP', 'PC', 'PUSH1', 'PUSH2', 'PUSH3', 'PUSH4', 'PUSH5', 'PUSH6', 'PUSH7', 'PUSH8', 'PUSH9', 'PUSH10', 'PUSH11', 'PUSH12', 'PUSH13', 'PUSH14', 'PUSH15', 'PUSH16', 'PUSH17', 'PUSH18', 'PUSH19', 'PUSH20', 'PUSH21', 'PUSH22', 'PUSH23', 'PUSH24', 'PUSH25', 'PUSH26', 'PUSH27', 'PUSH28', 'PUSH29', 'PUSH30', 'PUSH31', 'PUSH32', 'DUP1', 'DUP2', 'DUP3', 'DUP4', 'DUP5', 'DUP6', 'DUP7', 'DUP8', 'DUP9', 'DUP10', 'DUP11', 'DUP12', 'DUP13', 'DUP14', 'DUP15', 'DUP16', 'SWAP1', 'SWAP2', 'SWAP3', 'SWAP4', 'SWAP5', 'SWAP6', 'SWAP7', 'SWAP8', 'SWAP9', 'SWAP10', 'SWAP11', 'SWAP12', 'SWAP13', 'SWAP14', 'SWAP15', 'SWAP16', 'LOGO', 'LOG1', 'LOG2', 'LOG3', 'LOG4', 'PUSH', 'DUP', 'SWAP']\n",
    "node_attrs = node_types + labels\n",
    "class MyOwnDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        self.Ngraph = 65\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        ngraph = self.Ngraph\n",
    "        vers = [f'{idx}.ver' for idx in range(ngraph)]\n",
    "        edgs = [f'{idx}.edg' for idx in range(ngraph)]\n",
    "        bugs = [f'{idx}.type' for idx in range(ngraph)]\n",
    "        return vers + edgs + bugs\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        ngraph = self.Ngraph\n",
    "        graphs = [f'{idx}.grap' for idx in range(ngraph)]\n",
    "        return graphs\n",
    "\n",
    "    def download(self):\n",
    "        #  download the source file to `self.raw_dir`.\n",
    "        pass\n",
    "        print(\"in download\")\n",
    "        # raise RuntimeError(\"in download\")\n",
    "\n",
    "    def process(self):\n",
    "        self.exist_processed_file_names = []\n",
    "        for i, j, files in os.walk(self.processed_dir):\n",
    "            self.exist_processed_file_names = files\n",
    "            break\n",
    "\n",
    "        for f in self.processed_file_names:\n",
    "          if f not in self.exist_processed_file_names:\n",
    "            print(f\"process new file {f}\")\n",
    "            out_path = os.path.join(self.processed_dir, f)\n",
    "            data = self._process_per_graph(f)\n",
    "            torch.save(data, out_path)\n",
    "    \n",
    "    def _process_per_graph(self, f):\n",
    "        idx = f[:-5]\n",
    "        verPath = os.path.join(self.raw_dir, idx+'.ver')\n",
    "        edgPath = os.path.join(self.raw_dir, idx+'.edg')\n",
    "        bugPath = os.path.join(self.raw_dir, idx+'.type')\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        x = []\n",
    "        for line in open(edgPath, \"r\"):\n",
    "            line = line.strip('\\n')\n",
    "            line = line.replace(' ', '').split(',')\n",
    "            link = [int(line[0]), int(line[1])]\n",
    "            edge_index.append(link)\n",
    "            if line[2] == 'exec':\n",
    "                attr = [0, int(line[3])]\n",
    "            else:\n",
    "                attr = [1, int(line[3])]\n",
    "            edge_attr.append(attr)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "        for line in open(verPath, \"r\"):\n",
    "            line = line.strip('\\n')\n",
    "            attrOneHot = [0 for i in range(len(node_attrs))]\n",
    "            line = line.replace(' ', '').replace('\\'','')\n",
    "            attr_begin = line.index('[')+1\n",
    "            nodeType = line[:attr_begin-1].split(',')[1]\n",
    "            attrList = line[attr_begin:-1].split(',')\n",
    "            if '' in attrList:\n",
    "                attrList.remove('')\n",
    "            attrList.append(nodeType)\n",
    "            for attr in attrList:\n",
    "                idx = node_attrs.index(attr)\n",
    "                attrOneHot[idx] = 1\n",
    "            x.append(attrOneHot)\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        for line in open(bugPath, \"r\"):\n",
    "          line = line.strip('\\n')\n",
    "          y = torch.tensor(int(line), dtype=torch.long)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        return data\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))\n",
    "        return data\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.getitem(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayerGNN(torch.nn.Module):\n",
    "  def __init__(self, edge_dim=2, in_ch=172, hidden_ch=200, n_layers=2, drop_rate=0.5, JK=\"last\", residual=False):\n",
    "    super(MulLayerGNN, self).__init__()\n",
    "    self.n_layers = n_layers\n",
    "    self.drop_rate = drop_rate\n",
    "    self.JK = JK\n",
    "    self.residual = residual\n",
    "    self.convs = torch.nn.ModuleList()\n",
    "    self.batch_norms = torch.nn.ModuleList()\n",
    "    # self.convs.append(nn.GINEConv(\n",
    "    #       nn.Sequential('x', [(nn.Linear(in_ch, hidden_ch), 'x->x'),\n",
    "    #                           (torch.nn.ReLU(), 'x->x'),\n",
    "    #                           (nn.Linear(hidden_ch, hidden_ch), 'x->x')]),\n",
    "    #                     edge_dim=edge_dim))\n",
    "    self.convs.append(nn.GINEConv(\n",
    "        torch.nn.Sequential(\n",
    "            nn.Linear(in_ch, hidden_ch),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch)),\n",
    "        edge_dim=edge_dim))\n",
    "    self.batch_norms.append(torch.nn.BatchNorm1d(hidden_ch))\n",
    "    \n",
    "    for layer in range(1, n_layers):\n",
    "      self.convs.append(nn.GINEConv(\n",
    "          torch.nn.Sequential(\n",
    "              nn.Linear(hidden_ch, hidden_ch),\n",
    "              torch.nn.ReLU(),\n",
    "              nn.Linear(hidden_ch, hidden_ch)),\n",
    "          edge_dim=edge_dim))\n",
    "      self.batch_norms.append(torch.nn.BatchNorm1d(hidden_ch))\n",
    "  def forward(self, data):\n",
    "      x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "      h_list = [x]\n",
    "      for layer in range(self.n_layers):\n",
    "        h = self.convs[layer](x=h_list[layer], edge_index=edge_index, edge_attr=edge_attr)\n",
    "        h = self.batch_norms[layer](h)\n",
    "        if layer == self.n_layers - 1:\n",
    "          # remove relu for the last layer\n",
    "          h = F.dropout(h, self.drop_rate, training=self.training)\n",
    "        else:\n",
    "          h = F.dropout(F.relu(h), self.drop_rate, training=self.training)\n",
    "        if self.residual and layer != 0:\n",
    "          h += h_list[layer]\n",
    "        h_list.append(h)\n",
    "      if self.JK == \"last\":\n",
    "        node_representation = h_list[-1]\n",
    "      elif self.JK == \"sum\":\n",
    "        node_representation = 0\n",
    "        for layer in range(self.num_layers + 1):\n",
    "          node_representation += h_list[layer]\n",
    "      return node_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VulNet(torch.nn.Module):\n",
    "  def __init__(self, grah_dim=10, n_layers=3, edge_dim=2, in_ch=172, hidden_ch=200, residual=False, drop_rate=0, JK=\"last\", graph_pooling=\"sum\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      grah_dim    output dim of graph representaion vector, when used to classify, it's classes number\n",
    "      n_layers    how many node embedding layers eg.GCNConv/GINEConv\n",
    "      edge_dim    dim of edge vector\n",
    "      in_ch       dim of node init vector\n",
    "      hidden_ch   dim of node representaion vector\n",
    "      residual    adding residual connection or not. Defaults to False.\n",
    "      drop_rate   dropout rate. Defaults to 0.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(12345)\n",
    "    super(VulNet, self).__init__()\n",
    "\n",
    "    self.grah_dim = grah_dim\n",
    "    self.n_layers = n_layers\n",
    "    self.hidden_ch = hidden_ch\n",
    "    self.residual = residual\n",
    "    self.drop_rate = drop_rate\n",
    "    self.JK = JK\n",
    "\n",
    "    if self.n_layers < 2:\n",
    "      raise ValueError(\"argument 'n_layers':  Number of GNN layers must be greater than 1.\")\n",
    "    \n",
    "    self.gnn_body = MulLayerGNN(edge_dim, in_ch, hidden_ch, n_layers, drop_rate, JK, residual)\n",
    "    if graph_pooling == \"sum\":\n",
    "      self.pool = nn.global_add_pool\n",
    "    elif graph_pooling == \"mean\":\n",
    "      self.pool = nn.global_mean_pool\n",
    "    elif graph_pooling == \"max\":\n",
    "      self.pool = nn.global_max_pool\n",
    "    elif graph_pooling == \"attention\":\n",
    "      self.pool = nn.GlobalAttention(gate_nn=torch.nn.Sequential(\n",
    "                nn.Linear(hidden_ch, hidden_ch), torch.nn.BatchNorm1d(hidden_ch), torch.nn.ReLU(), nn.Linear(hidden_ch, 1)))\n",
    "    elif graph_pooling == \"set2set\":\n",
    "      self.pool = nn.Set2Set(hidden_ch, processing_steps=2)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "    if graph_pooling == \"set2set\":\n",
    "      self.graph_linear = nn.Linear(2*self.hidden_ch, self.grah_dim)\n",
    "    else:\n",
    "      self.graph_linear = nn.Linear(self.hidden_ch, self.grah_dim)\n",
    "  def forward(self, data):\n",
    "    h_node = self.gnn_body(data)\n",
    "    h_graph = self.pool(h_node, data.batch)\n",
    "\n",
    "    return self.graph_linear(h_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = MyOwnDataset(\"./data/smartbugs\").shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples:45 | test samples:20\n"
     ]
    }
   ],
   "source": [
    "train_n = int(0.7 * len(dst))\n",
    "train_dst = dst[:train_n]\n",
    "test_dst = dst[train_n:]\n",
    "print(f'train samples:{len(train_dst)} | test samples:{len(test_dst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = train_dst[0].x.size()[1]\n",
    "out_ch = 3\n",
    "edge_dim=train_dst[0].edge_attr.size()[1]\n",
    "train_loader = DataLoader(train_dst, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dst, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VulNet(grah_dim=out_ch, n_layers=2, edge_dim=edge_dim, in_ch=in_ch, hidden_ch=200, residual=True, drop_rate=0.1, JK=\"last\",graph_pooling=\"sum\").to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "Epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "begin = time.time()\n",
    "for epoch in range(Epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    train_n = 0\n",
    "    test_n = 0\n",
    "\n",
    "    net.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        train_pred = net(data.to(device)) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = criterion(train_pred, data.y.to(device)) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "        batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.data.cpu().numpy(), axis=1) == data.y.cpu().numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "        train_n += train_pred.size()[0]\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = net(data.to(device))\n",
    "            batch_loss = criterion(test_pred, data.y.to(device))\n",
    "\n",
    "            test_acc += np.sum(np.argmax(test_pred.data.cpu().numpy(), axis=1) == data.y.cpu().numpy())\n",
    "            test_loss += batch_loss.item()\n",
    "            test_n += test_pred.size()[0]\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, Epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/len(train_dst), train_loss/len(train_dst), \\\n",
    "             test_acc/len(test_dst), test_loss/len(test_dst)))\n",
    "print(\"total %2.2f sec(s)\" % (time.time()-begin)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_loader):\n",
    "    break\n",
    "pred = net(data.cuda()).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 0.95)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(np.argmax(pred.cpu().data.numpy(), axis=1) == data.cpu().y.numpy())\n",
    "acc, acc*1.0/pred.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 2, 1, 0, 2, 1, 1, 0, 2, 1, 2, 2, 2, 1, 0, 2, 1, 0, 2, 2],\n",
       "       dtype=int64),\n",
       " array([2, 2, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2, 1, 0, 2, 1, 0, 2, 2],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred.cpu().data.numpy(), axis=1),data.cpu().y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "289b4d7ddcb0894f1ed982e51939adc2194da2b15a59e3bc72b9d1eee8efce2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pye')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
