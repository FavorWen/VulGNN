{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data, InMemoryDataset\n",
    "import torch_geometric.nn as nn\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "labels = ['addr_contract', 'caller', 'msgvalue', 'balance', 'call_data', 'blk', 'mdata', 'sdata', 'create', 'call', 'callcode', 'delegatecall', 'create2', 'staticcall', 'cal_res', 'comp_res', 'bit_res', 'size', 'code', 'gas', 'return', 'coinbase', 'gasremain', 'revert', 'selfdestruct', 'memory', 'storage', 'flowcontrol']\n",
    "node_types = ['ADDRESS', 'ORIGIN', 'CALLER', 'CALLVALUE', 'BALANCE', 'SELFBALANCE', 'CALLDATALOAD', 'CALLDATACOPY', 'BLOCKHASH', 'TIMESTAMP', 'NUMBER', 'DIFFICULTY', 'BASEFEE', 'MLOAD', 'SLOAD', 'CREATE', 'CALL', 'CALLCODE', 'DELEGATECALL', 'CREATE2', 'STATICCALL', 'ADD', 'MUL', 'SUB', 'EXP', 'LT', 'GT', 'SLT', 'SGT', 'EQ', 'ISZERO', 'AND', 'OR', 'XOR', 'NOT', 'SHL', 'CALLDATASIZE', 'CODESIZE', 'EXTCODESIZE', 'RETURNDATASIZE', 'MSIZE', 'CODECOPY', 'EXTCODECOPY', 'EXTCODEHASH', 'GASPRICE', 'GASLIMIT', 'RETURNDATACOPY', 'RETURN', 'COINBASE', 'GAS', 'REVERT', 'SELFDESTRUCT', 'MSTORE', 'MSTORE8', 'SSTORE', 'JUMP', 'JUMPI', 'JUMPDEST', 'STOP', 'DIV', 'SDIV', 'MOD', 'SMOD', 'ADDMOD', 'SIGNEXTEND', 'BYTE', 'SHR', 'SAR', 'SHA3', 'CHAINID', 'POP', 'PC', 'PUSH1', 'PUSH2', 'PUSH3', 'PUSH4', 'PUSH5', 'PUSH6', 'PUSH7', 'PUSH8', 'PUSH9', 'PUSH10', 'PUSH11', 'PUSH12', 'PUSH13', 'PUSH14', 'PUSH15', 'PUSH16', 'PUSH17', 'PUSH18', 'PUSH19', 'PUSH20', 'PUSH21', 'PUSH22', 'PUSH23', 'PUSH24', 'PUSH25', 'PUSH26', 'PUSH27', 'PUSH28', 'PUSH29', 'PUSH30', 'PUSH31', 'PUSH32', 'DUP1', 'DUP2', 'DUP3', 'DUP4', 'DUP5', 'DUP6', 'DUP7', 'DUP8', 'DUP9', 'DUP10', 'DUP11', 'DUP12', 'DUP13', 'DUP14', 'DUP15', 'DUP16', 'SWAP1', 'SWAP2', 'SWAP3', 'SWAP4', 'SWAP5', 'SWAP6', 'SWAP7', 'SWAP8', 'SWAP9', 'SWAP10', 'SWAP11', 'SWAP12', 'SWAP13', 'SWAP14', 'SWAP15', 'SWAP16', 'LOGO', 'LOG1', 'LOG2', 'LOG3', 'LOG4', 'PUSH', 'DUP', 'SWAP']\n",
    "node_attrs = node_types + labels\n",
    "class MyOwnDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        self.Ngraph = 44\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        ngraph = self.Ngraph\n",
    "        vers = [f'{idx}.ver' for idx in range(ngraph)]\n",
    "        edgs = [f'{idx}.edg' for idx in range(ngraph)]\n",
    "        bugs = [f'{idx}.type' for idx in range(ngraph)]\n",
    "        return vers + edgs + bugs\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        ngraph = self.Ngraph\n",
    "        graphs = [f'{idx}.grap' for idx in range(ngraph)]\n",
    "        return graphs\n",
    "\n",
    "    def download(self):\n",
    "        #  download the source file to `self.raw_dir`.\n",
    "        pass\n",
    "        print(\"in download\")\n",
    "        # raise RuntimeError(\"in download\")\n",
    "\n",
    "    def process(self):\n",
    "        self.exist_processed_file_names = []\n",
    "        for i, j, files in os.walk(self.processed_dir):\n",
    "            self.exist_processed_file_names = files\n",
    "            break\n",
    "\n",
    "        for f in self.processed_file_names:\n",
    "          if f not in self.exist_processed_file_names:\n",
    "            print(f\"process new file {f}\")\n",
    "            out_path = os.path.join(self.processed_dir, f)\n",
    "            data = self._process_per_graph(f)\n",
    "            torch.save(data, out_path)\n",
    "    \n",
    "    def _process_per_graph(self, f):\n",
    "        idx = f[:-5]\n",
    "        verPath = os.path.join(self.raw_dir, idx+'.ver')\n",
    "        edgPath = os.path.join(self.raw_dir, idx+'.edg')\n",
    "        bugPath = os.path.join(self.raw_dir, idx+'.type')\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        x = []\n",
    "        for line in open(edgPath, \"r\"):\n",
    "            line = line.strip('\\n')\n",
    "            line = line.replace(' ', '').split(',')\n",
    "            link = [int(line[0]), int(line[1])]\n",
    "            edge_index.append(link)\n",
    "            if line[2] == 'exec':\n",
    "                attr = [0, int(line[3])]\n",
    "            else:\n",
    "                attr = [1, int(line[3])]\n",
    "            edge_attr.append(attr)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "        for line in open(verPath, \"r\"):\n",
    "            line = line.strip('\\n')\n",
    "            attrOneHot = [0 for i in range(len(node_attrs))]\n",
    "            line = line.replace(' ', '').replace('\\'','')\n",
    "            attr_begin = line.index('[')+1\n",
    "            nodeType = line[:attr_begin-1].split(',')[1]\n",
    "            attrList = line[attr_begin:-1].split(',')\n",
    "            if '' in attrList:\n",
    "                attrList.remove('')\n",
    "            attrList.append(nodeType)\n",
    "            for attr in attrList:\n",
    "                idx = node_attrs.index(attr)\n",
    "                attrOneHot[idx] = 1\n",
    "            x.append(attrOneHot)\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        for line in open(bugPath, \"r\"):\n",
    "          line = line.strip('\\n')\n",
    "          y = torch.tensor(int(line), dtype=torch.long)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        return data\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))\n",
    "        return data\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.getitem(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayerGNN(torch.nn.Module):\n",
    "  def __init__(self, edge_dim=2, in_ch=172, hidden_ch=200, n_layers=2, drop_rate=0.5, JK=\"last\", residual=False):\n",
    "    super(MulLayerGNN, self).__init__()\n",
    "    self.n_layers = n_layers\n",
    "    self.drop_rate = drop_rate\n",
    "    self.JK = JK\n",
    "    self.residual = residual\n",
    "    self.convs = torch.nn.ModuleList()\n",
    "    self.batch_norms = torch.nn.ModuleList()\n",
    "    # self.convs.append(nn.GINEConv(\n",
    "    #       nn.Sequential('x', [(nn.Linear(in_ch, hidden_ch), 'x->x'),\n",
    "    #                           (torch.nn.ReLU(), 'x->x'),\n",
    "    #                           (nn.Linear(hidden_ch, hidden_ch), 'x->x')]),\n",
    "    #                     edge_dim=edge_dim))\n",
    "    self.convs.append(nn.GINEConv(\n",
    "        torch.nn.Sequential(\n",
    "            nn.Linear(in_ch, hidden_ch),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Linear(hidden_ch, hidden_ch)),\n",
    "        edge_dim=edge_dim))\n",
    "    self.batch_norms.append(torch.nn.BatchNorm1d(hidden_ch))\n",
    "    \n",
    "    for layer in range(1, n_layers):\n",
    "      self.convs.append(nn.GINEConv(\n",
    "          torch.nn.Sequential(\n",
    "              nn.Linear(hidden_ch, hidden_ch),\n",
    "              torch.nn.ReLU(),\n",
    "              nn.Linear(hidden_ch, hidden_ch)),\n",
    "          edge_dim=edge_dim))\n",
    "      self.batch_norms.append(torch.nn.BatchNorm1d(hidden_ch))\n",
    "  def forward(self, data):\n",
    "      x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "      h_list = [x]\n",
    "      for layer in range(self.n_layers):\n",
    "        h = self.convs[layer](x=h_list[layer], edge_index=edge_index, edge_attr=edge_attr)\n",
    "        h = self.batch_norms[layer](h)\n",
    "        if layer == self.n_layers - 1:\n",
    "          # remove relu for the last layer\n",
    "          h = F.dropout(h, self.drop_rate, training=self.training)\n",
    "        else:\n",
    "          h = F.dropout(F.relu(h), self.drop_rate, training=self.training)\n",
    "        if self.residual:\n",
    "          h += h_list[layer]\n",
    "        h_list.append(h)\n",
    "      if self.JK == \"last\":\n",
    "        node_representation = h_list[-1]\n",
    "      elif self.JK == \"sum\":\n",
    "        node_representation = 0\n",
    "        for layer in range(self.num_layers + 1):\n",
    "          node_representation += h_list[layer]\n",
    "      return node_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VulNet(torch.nn.Module):\n",
    "  def __init__(self, grah_dim=10, n_layers=3, edge_dim=2, in_ch=172, hidden_ch=200, residual=False, drop_rate=0, JK=\"last\", graph_pooling=\"sum\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      grah_dim    output dim of graph representaion vector, when used to classify, it's classes number\n",
    "      n_layers    how many node embedding layers eg.GCNConv/GINEConv\n",
    "      edge_dim    dim of edge vector\n",
    "      in_ch       dim of node init vector\n",
    "      hidden_ch   dim of node representaion vector\n",
    "      residual    adding residual connection or not. Defaults to False.\n",
    "      drop_rate   dropout rate. Defaults to 0.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(12345)\n",
    "    super(VulNet, self).__init__()\n",
    "\n",
    "    self.grah_dim = grah_dim\n",
    "    self.n_layers = n_layers\n",
    "    self.hidden_ch = hidden_ch\n",
    "    self.residual = residual\n",
    "    self.drop_rate = drop_rate\n",
    "    self.JK = JK\n",
    "\n",
    "    if self.n_layers < 2:\n",
    "      raise ValueError(\"argument 'n_layers':  Number of GNN layers must be greater than 1.\")\n",
    "    \n",
    "    self.gnn_body = MulLayerGNN(edge_dim, in_ch, hidden_ch, n_layers, drop_rate, JK, residual)\n",
    "    if graph_pooling == \"sum\":\n",
    "      self.pool = nn.global_add_pool\n",
    "    elif graph_pooling == \"mean\":\n",
    "      self.pool = nn.global_mean_pool\n",
    "    elif graph_pooling == \"max\":\n",
    "      self.pool = nn.global_max_pool\n",
    "    elif graph_pooling == \"attention\":\n",
    "      self.pool = nn.GlobalAttention(gate_nn=torch.nn.Sequential(\n",
    "                nn.Linear(hidden_ch, hidden_ch), torch.nn.BatchNorm1d(hidden_ch), torch.nn.ReLU(), nn.Linear(hidden_ch, 1)))\n",
    "    elif graph_pooling == \"set2set\":\n",
    "      self.pool = nn.Set2Set(hidden_ch, processing_steps=2)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "    if graph_pooling == \"set2set\":\n",
    "      self.graph_linear = nn.Linear(2*self.hidden_ch, self.grah_dim)\n",
    "    else:\n",
    "      self.graph_linear = nn.Linear(self.hidden_ch, self.grah_dim)\n",
    "  def forward(self, data):\n",
    "    h_node = self.gnn_body(data)\n",
    "    h_graph = self.pool(h_node, data.batch)\n",
    "\n",
    "    return self.graph_linear(h_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = MyOwnDataset(\"../data/smartbugs\").shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = int(0.65 * len(dst))\n",
    "train_dst = dst[:train_n]\n",
    "test_dst = dst[train_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = train_dst[0].x.size()[1]\n",
    "out_ch = 2\n",
    "edge_dim=train_dst[0].edge_attr.size()[1]\n",
    "train_loader = DataLoader(train_dst, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dst, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VulNet(out_ch, 5, edge_dim, in_ch, 200, False, 0, \"last\",\"sum\").cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "Epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/100] 0.36 sec(s) Train Acc: 0.607143 Loss: 0.117286 | Val Acc: 0.500000 loss: 65.445747\n",
      "[002/100] 0.14 sec(s) Train Acc: 0.642857 Loss: 0.702744 | Val Acc: 0.500000 loss: 452.025116\n",
      "[003/100] 0.15 sec(s) Train Acc: 0.642857 Loss: 0.870198 | Val Acc: 0.500000 loss: 342.612946\n",
      "[004/100] 0.14 sec(s) Train Acc: 0.750000 Loss: 0.166647 | Val Acc: 0.500000 loss: 5791.198730\n",
      "[005/100] 0.16 sec(s) Train Acc: 0.821429 Loss: 0.126328 | Val Acc: 0.500000 loss: 5958.723145\n",
      "[006/100] 0.15 sec(s) Train Acc: 0.607143 Loss: 0.131847 | Val Acc: 0.500000 loss: 3648.485840\n",
      "[007/100] 0.16 sec(s) Train Acc: 0.892857 Loss: 0.045878 | Val Acc: 0.500000 loss: 1105.562256\n",
      "[008/100] 0.14 sec(s) Train Acc: 0.857143 Loss: 0.016552 | Val Acc: 0.500000 loss: 363.233002\n",
      "[009/100] 0.15 sec(s) Train Acc: 0.821429 Loss: 0.062391 | Val Acc: 0.500000 loss: 345.570251\n",
      "[010/100] 0.14 sec(s) Train Acc: 0.785714 Loss: 0.083648 | Val Acc: 0.500000 loss: 123.556999\n",
      "[011/100] 0.18 sec(s) Train Acc: 0.857143 Loss: 0.049599 | Val Acc: 0.500000 loss: 86.928009\n",
      "[012/100] 0.17 sec(s) Train Acc: 0.857143 Loss: 0.018198 | Val Acc: 0.500000 loss: 344.628601\n",
      "[013/100] 0.15 sec(s) Train Acc: 0.928571 Loss: 0.005953 | Val Acc: 0.500000 loss: 295.881042\n",
      "[014/100] 0.16 sec(s) Train Acc: 0.928571 Loss: 0.016743 | Val Acc: 0.500000 loss: 113.906273\n",
      "[015/100] 0.17 sec(s) Train Acc: 0.928571 Loss: 0.018269 | Val Acc: 0.500000 loss: 14.593220\n",
      "[016/100] 0.14 sec(s) Train Acc: 0.964286 Loss: 0.006345 | Val Acc: 0.500000 loss: 47.428528\n",
      "[017/100] 0.14 sec(s) Train Acc: 0.964286 Loss: 0.002183 | Val Acc: 0.500000 loss: 54.443306\n",
      "[018/100] 0.18 sec(s) Train Acc: 0.928571 Loss: 0.004865 | Val Acc: 0.500000 loss: 42.095249\n",
      "[019/100] 0.16 sec(s) Train Acc: 0.964286 Loss: 0.004038 | Val Acc: 0.500000 loss: 24.046408\n",
      "[020/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000802 | Val Acc: 0.500000 loss: 9.855135\n",
      "[021/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000030 | Val Acc: 0.437500 loss: 2.537721\n",
      "[022/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000005 | Val Acc: 0.437500 loss: 22.288183\n",
      "[023/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000013 | Val Acc: 0.500000 loss: 34.422371\n",
      "[024/100] 0.17 sec(s) Train Acc: 1.000000 Loss: 0.000133 | Val Acc: 0.500000 loss: 38.495880\n",
      "[025/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000190 | Val Acc: 0.500000 loss: 38.681160\n",
      "[026/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000692 | Val Acc: 0.500000 loss: 33.157589\n",
      "[027/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000585 | Val Acc: 0.500000 loss: 24.946550\n",
      "[028/100] 0.13 sec(s) Train Acc: 1.000000 Loss: 0.000143 | Val Acc: 0.500000 loss: 18.169601\n",
      "[029/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000035 | Val Acc: 0.437500 loss: 12.937907\n",
      "[030/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000014 | Val Acc: 0.437500 loss: 8.849586\n",
      "[031/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000008 | Val Acc: 0.375000 loss: 5.599366\n",
      "[032/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000005 | Val Acc: 0.437500 loss: 3.092326\n",
      "[033/100] 0.13 sec(s) Train Acc: 1.000000 Loss: 0.000004 | Val Acc: 0.500000 loss: 1.203474\n",
      "[034/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000004 | Val Acc: 0.812500 loss: 0.238921\n",
      "[035/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000006 | Val Acc: 0.750000 loss: 0.311859\n",
      "[036/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000010 | Val Acc: 0.687500 loss: 0.467388\n",
      "[037/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000010 | Val Acc: 0.500000 loss: 0.738711\n",
      "[038/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000008 | Val Acc: 0.500000 loss: 0.944803\n",
      "[039/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000008 | Val Acc: 0.500000 loss: 1.030952\n",
      "[040/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000008 | Val Acc: 0.500000 loss: 1.051129\n",
      "[041/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000007 | Val Acc: 0.500000 loss: 1.018498\n",
      "[042/100] 0.13 sec(s) Train Acc: 1.000000 Loss: 0.000005 | Val Acc: 0.500000 loss: 0.929238\n",
      "[043/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000003 | Val Acc: 0.500000 loss: 0.806142\n",
      "[044/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000002 | Val Acc: 0.562500 loss: 0.683941\n",
      "[045/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000002 | Val Acc: 0.687500 loss: 0.581479\n",
      "[046/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000002 | Val Acc: 0.687500 loss: 0.505557\n",
      "[047/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000002 | Val Acc: 0.687500 loss: 0.450011\n",
      "[048/100] 0.13 sec(s) Train Acc: 1.000000 Loss: 0.000002 | Val Acc: 0.687500 loss: 0.410967\n",
      "[049/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.687500 loss: 0.379069\n",
      "[050/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.687500 loss: 0.348931\n",
      "[051/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.687500 loss: 0.320311\n",
      "[052/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.812500 loss: 0.294552\n",
      "[053/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.812500 loss: 0.273373\n",
      "[054/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.812500 loss: 0.254262\n",
      "[055/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.875000 loss: 0.237884\n",
      "[056/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.875000 loss: 0.223143\n",
      "[057/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.875000 loss: 0.208317\n",
      "[058/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000001 | Val Acc: 0.875000 loss: 0.193379\n",
      "[059/100] 0.17 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.875000 loss: 0.178687\n",
      "[060/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.875000 loss: 0.164883\n",
      "[061/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.875000 loss: 0.152620\n",
      "[062/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.875000 loss: 0.142782\n",
      "[063/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.937500 loss: 0.135729\n",
      "[064/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.875000 loss: 0.131398\n",
      "[065/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.875000 loss: 0.129200\n",
      "[066/100] 0.13 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.875000 loss: 0.128452\n",
      "[067/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.812500 loss: 0.128751\n",
      "[068/100] 0.13 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.812500 loss: 0.129682\n",
      "[069/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.812500 loss: 0.130955\n",
      "[070/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.132357\n",
      "[071/100] 0.18 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.133758\n",
      "[072/100] 0.18 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.135151\n",
      "[073/100] 0.17 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.136493\n",
      "[074/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.137790\n",
      "[075/100] 0.14 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.138990\n",
      "[076/100] 0.34 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.140082\n",
      "[077/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.141078\n",
      "[078/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.141971\n",
      "[079/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.142864\n",
      "[080/100] 0.18 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.143772\n",
      "[081/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.144635\n",
      "[082/100] 0.19 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.145485\n",
      "[083/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.146328\n",
      "[084/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.147179\n",
      "[085/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.147992\n",
      "[086/100] 0.15 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.148772\n",
      "[087/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.149532\n",
      "[088/100] 0.18 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.150301\n",
      "[089/100] 0.38 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.151074\n",
      "[090/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.151844\n",
      "[091/100] 0.19 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.152627\n",
      "[092/100] 0.17 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.153424\n",
      "[093/100] 0.18 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.154215\n",
      "[094/100] 0.19 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.154994\n",
      "[095/100] 0.17 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.155758\n",
      "[096/100] 0.18 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.156518\n",
      "[097/100] 0.20 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.157267\n",
      "[098/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.157995\n",
      "[099/100] 0.19 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.158702\n",
      "[100/100] 0.16 sec(s) Train Acc: 1.000000 Loss: 0.000000 | Val Acc: 0.750000 loss: 0.159371\n",
      "total 16.19 sec(s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "begin = time.time()\n",
    "for epoch in range(Epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    train_n = 0\n",
    "    test_n = 0\n",
    "\n",
    "    net.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        train_pred = net(data.cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = criterion(train_pred, data.y.cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "        batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data.y.cpu().numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "        train_n += train_pred.size()[0]\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = net(data.cuda())\n",
    "            batch_loss = criterion(test_pred, data.y.cuda())\n",
    "\n",
    "            test_acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) == data.y.cpu().numpy())\n",
    "            test_loss += batch_loss.item()\n",
    "            test_n += test_pred.size()[0]\n",
    "\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, Epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/len(train_dst), train_loss/len(train_dst), \\\n",
    "             test_acc/len(test_dst), test_loss/len(test_dst)))\n",
    "print(\"total %2.2f sec(s)\" % (time.time()-begin)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_loader):\n",
    "    break\n",
    "pred = net(data.cuda()).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(np.argmax(pred.cpu().data.numpy(), axis=1) == data.cpu().y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], dtype=int64),\n",
       " array([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int64))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred.cpu().data.numpy(), axis=1),data.cpu().y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 16)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n, test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "634ffc23e07d1ff39bbbc78f82419442f958d394482c657a5bd6f3ea84c523b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pye')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
